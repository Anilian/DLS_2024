{"cells":[{"cell_type":"markdown","metadata":{"id":"Ot3c4fjZwC4T"},"source":["<img src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500, height=450>\n","<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"]},{"cell_type":"markdown","metadata":{"id":"P2JdzEXmwRU5"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"Fc8iHXIVwDwj"},"source":["***Some parts of the notebook are almost the copy of [ mmta-team course](https://github.com/mmta-team/mmta_fall_2020). Special thanks to mmta-team for making them publicly available. [Original notebook](https://github.com/mmta-team/mmta_fall_2020/blob/master/tasks/01_word_embeddings/task_word_embeddings.ipynb).***"]},{"cell_type":"markdown","metadata":{"id":"7D0wm5jt6j0U"},"source":["<b> Прочитайте семинар, пожалуйста, для успешного выполнения домашнего задания. В конце ноутка напишите свой вывод. Работа без вывода оценивается ниже."]},{"cell_type":"markdown","metadata":{"id":"BIWqBuEa6j0b"},"source":["## Задача поиска схожих по смыслу предложений"]},{"cell_type":"markdown","metadata":{"id":"NUkwMPLA6j0g"},"source":["Мы будем ранжировать вопросы [StackOverflow](https://stackoverflow.com) на основе семантического векторного представления "]},{"cell_type":"markdown","metadata":{"id":"dNRXIEfu5a3Q"},"source":["До этого в курсе не было речи про задачу ранжировния, поэтому введем математическую формулировку"]},{"cell_type":"markdown","metadata":{"id":"uS9FwWNd5a3S"},"source":["## Задача ранжирования(Learning to Rank)"]},{"cell_type":"markdown","metadata":{"id":"wdwY9-f75a3T"},"source":["* $X$ - множество объектов\n","* $X^l = \\{x_1, x_2, ..., x_l\\}$ - обучающая выборка\n","<br>На обучающей выборке задан порядок между некоторыми элементами, то есть нам известно, что некий объект выборки более релевантный для нас, чем другой:\n","* $i \\prec j$ - порядок пары индексов объектов на выборке $X^l$ c индексами $i$ и $j$\n","### Задача:\n","построить ранжирующую функцию $a$ : $X \\rightarrow R$ такую, что\n","$$i \\prec j \\Rightarrow a(x_i) < a(x_j)$$"]},{"cell_type":"markdown","metadata":{"id":"WG2IGBsh5a3U"},"source":["<img src=\"https://d25skit2l41vkl.cloudfront.net/wp-content/uploads/2016/12/Featured-Image.jpg\" width=500, height=450>"]},{"cell_type":"markdown","metadata":{"id":"MQk_rolFwT_h"},"source":["### Embeddings"]},{"cell_type":"markdown","metadata":{"id":"xUe1PGXn6j0l"},"source":["Будем использовать предобученные векторные представления слов на постах Stack Overflow.<br>\n","[A word2vec model trained on Stack Overflow posts](https://github.com/vefstathiou/SO_word2vec)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"mYkI54Y-rk7a"},"outputs":[{"name":"stderr","output_type":"stream","text":["\"wget\" �� ���� ����७��� ��� ���譥�\n","��������, �ᯮ��塞�� �ணࠬ��� ��� ������ 䠩���.\n"]}],"source":["!wget https://zenodo.org/record/1199620/files/SO_vectors_200.bin?download=1"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"O8YJTOYv6j0s"},"outputs":[],"source":["from gensim.models.keyedvectors import KeyedVectors\n","wv_embeddings = KeyedVectors.load_word2vec_format(\"SO_vectors_200.bin\", binary=True)"]},{"cell_type":"markdown","metadata":{"id":"aIcT_g-C6j1E"},"source":["#### Как пользоваться этими векторами?"]},{"cell_type":"markdown","metadata":{"id":"DWO5SPDY6j1G"},"source":["Посмотрим на примере одного слова, что из себя представляет embedding"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"KeSBlQfk6j1J","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["float32 (200,)\n"]}],"source":["word = 'dog'\n","if word in wv_embeddings:\n","    print(wv_embeddings[word].dtype, wv_embeddings[word].shape)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"T4Eq-D1qxpMJ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Num of words: 1787145\n"]}],"source":["print(f\"Num of words: {len(wv_embeddings.index_to_key)}\")"]},{"cell_type":"markdown","metadata":{"id":"ZT6NTCys6j1Q"},"source":["Найдем наиболее близкие слова к слову `dog`:"]},{"cell_type":"markdown","metadata":{"id":"n08z2PjMwC5o"},"source":["#### Вопрос 1:\n","* Входит ли слов `cat` топ-5 близких слов к слову `dog`? Какое место? "]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["[('animal', 0.8564180135726929),\n"," ('dogs', 0.7880866527557373),\n"," ('mammal', 0.7623804211616516),\n"," ('cats', 0.7621253728866577),\n"," ('animals', 0.760793924331665)]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["word_original = 'dog'\n","wv_embeddings.most_similar(positive=word_original, topn=5)"]},{"cell_type":"markdown","metadata":{},"source":["Слово \"cat\" не входит в ближайшие слова "]},{"cell_type":"markdown","metadata":{"id":"ai48-5vv6j1d"},"source":["### Векторные представления текста\n","\n","Перейдем от векторных представлений отдельных слов к векторным представлениям вопросов, как к **среднему** векторов всех слов в вопросе. Если для какого-то слова нет предобученного вектора, то его нужно пропустить. Если вопрос не содержит ни одного известного слова, то нужно вернуть нулевой вектор."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"EhNuxBJd6j1f"},"outputs":[{"name":"stderr","output_type":"stream","text":["<>:9: SyntaxWarning: invalid escape sequence '\\w'\n","<>:9: SyntaxWarning: invalid escape sequence '\\w'\n","C:\\Users\\Anna\\AppData\\Local\\Temp\\ipykernel_12312\\2760772041.py:9: SyntaxWarning: invalid escape sequence '\\w'\n","  return re.findall('\\w+', text)\n"]}],"source":["import numpy as np\n","import re\n","# you can use your tokenizer\n","# for example, from nltk.tokenize import WordPunctTokenizer\n","class MyTokenizer:\n","    def __init__(self):\n","        pass\n","    def tokenize(self, text):\n","        return re.findall('\\w+', text)\n","tokenizer = MyTokenizer()"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"YHcvu6186j1m"},"outputs":[],"source":["def question_to_vec(question, embeddings, tokenizer, dim=200):\n","    \"\"\"\n","        question: строка\n","        embeddings: наше векторное представление\n","        dim: размер любого вектора в нашем представлении\n","        \n","        return: векторное представление для вопроса\n","    \"\"\"\n","    embeddings_sentence = []\n","    words = tokenizer.tokenize(question)\n","    for w in words:\n","        if w in embeddings:\n","            embeddings_sentence.append(embeddings[w])\n","        else:\n","            continue\n","    if not embeddings_sentence: #Список пуст\n","        return np.zeros(dim)\n","    else:\n","        return np.mean(embeddings_sentence, axis=0)"]},{"cell_type":"markdown","metadata":{"id":"u5Q_4j7r6j1u"},"source":["Теперь у нас есть метод для создания векторного представления любого предложения."]},{"cell_type":"markdown","metadata":{"id":"EsJSNkhm6j1y"},"source":["#### Вопрос 2:\n","* Какая третья(с индексом 2) компонента вектора предложения `I love neural networks` (округлите до 2 знаков после запятой)?"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"a62r11cT6j10","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["-1.29\n"]}],"source":["question = 'I love neural networks'\n","emb = question_to_vec(question, wv_embeddings, tokenizer, dim=200)\n","print(round(emb[2], 2))"]},{"cell_type":"markdown","metadata":{"id":"Y60z4t6W6j16"},"source":["### Оценка близости текстов\n","\n","Представим, что мы используем идеальные векторные представления слов. Тогда косинусное расстояние между дублирующими предложениями должно быть меньше, чем между случайно взятыми предложениями. \n","\n","Сгенерируем для каждого из $N$ вопросов $R$ случайных отрицательных примеров и примешаем к ним также настоящие дубликаты. Для каждого вопроса будем ранжировать с помощью нашей модели $R + 1$ примеров и смотреть на позицию дубликата. Мы хотим, чтобы дубликат был первым в ранжированном списке.\n","\n","#### Hits@K\n","Первой простой метрикой будет количество корректных попаданий для какого-то $K$:\n","$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [rank\\_q_i^{'} \\le K],$$\n","* $\\begin{equation*}\n","[x < 0 ] \\equiv \n"," \\begin{cases}\n","   1, &x < 0\\\\\n","   0, &x \\geq 0\n"," \\end{cases}\n","\\end{equation*}$ - индикаторная функция\n","* $q_i$ - $i$-ый вопрос\n","* $q_i^{'}$ - его дубликат\n","* $rank\\_q_i^{'}$ - позиция дубликата в ранжированном списке ближайших предложений для вопроса $q_i$.\n","\n","#### DCG@K\n","Второй метрикой будет упрощенная DCG метрика, учитывающая порядок элементов в списке путем домножения релевантности элемента на вес равный обратному логарифму номера позиции::\n","$$ \\text{DCG@K} = \\frac{1}{N} \\sum_{i=1}^N\\frac{1}{\\log_2(1+rank\\_q_i^{'})}\\cdot[rank\\_q_i^{'} \\le K],$$\n","С такой метрикой модель штрафуется за большой ранк корректного ответа"]},{"cell_type":"markdown","metadata":{"id":"eHCnH-jw6j18"},"source":["#### Вопрос 3:\n","* Максимум `Hits@47 - DCG@1`?\n","\n","Ответ: 1\n","\n","Максимальное значение разницы между двумя функциями достигается, когда:\n","\n","Hits@47 = 1 (хотя бы один релевантный элемент попадает в топ-47)\n","DCG@1 = 0 (релевантный элемент не находится на первой позиции)\n","В этом случае, максимум разницы равен 1 - 0 = 1. "]},{"cell_type":"markdown","metadata":{"id":"_tFemBkP6j1-"},"source":["<img src='https://hsto.org/files/1c5/edf/dee/1c5edfdeebce4b71a86bdf986d9f88f2.jpg' width=400, height=200>"]},{"cell_type":"markdown","metadata":{"id":"0sUSxk866j1_"},"source":["#### Пример оценок\n","\n","Вычислим описанные выше метрики для игрушечного примера. \n","Пусть\n","* $N = 1$, $R = 3$\n","* <font color='green'>\"Что такое python?\"</font> - вопрос $q_1$\n","* <font color='red'>\"Что такое язык python?\"</font> - его дубликат $q_i^{'}$\n","\n","Пусть модель выдала следующий ранжированный список кандидатов:\n","\n","1. \"Как изучить с++?\"\n","2. <font color='red'>\"Что такое язык python?\"</font>\n","3. \"Хочу учить Java\"\n","4. \"Не понимаю Tensorflow\"\n","\n","$\\Rightarrow rank\\_q_i^{'} = 2$\n","\n","Вычислим метрику *Hits@K* для *K = 1, 4*:\n","\n","- [K = 1] $\\text{Hits@1} =  [rank\\_q_i^{'} \\le 1)] = 0$\n","- [K = 4] $\\text{Hits@4} =  [rank\\_q_i^{'} \\le 4] = 1$\n","\n","Вычислим метрику *DCG@K* для *K = 1, 4*:\n","- [K = 1] $\\text{DCG@1} = \\frac{1}{\\log_2(1+2)}\\cdot[2 \\le 1] = 0$\n","- [K = 4] $\\text{DCG@4} = \\frac{1}{\\log_2(1+2)}\\cdot[2 \\le 4] = \\frac{1}{\\log_2{3}}$"]},{"cell_type":"markdown","metadata":{"id":"B4L6HJJC6j2B"},"source":["#### Вопрос 4:\n","* Вычислите `DCG@10`, если $rank\\_q_i^{'} = 9$(округлите до одного знака после запятой)\n","\n","Ответ: [K = 10] $\\text{DCG@10} = \\frac{1}{\\log_2(1+9)}\\cdot[9 \\le 10] = \\frac{1}{\\log_2{10}}$"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.3\n"]}],"source":["import math\n","print(round(1 / math.log(10, 2), 1))"]},{"cell_type":"markdown","metadata":{"id":"J5xWOORI6j2F"},"source":["### HITS\\_COUNT и DCG\\_SCORE"]},{"cell_type":"markdown","metadata":{"id":"I1q9WQOx6j2H"},"source":["Каждая функция имеет два аргумента: $dup\\_ranks$ и $k$. $dup\\_ranks$ является списком, который содержит рейтинги дубликатов(их позиции в ранжированном списке). Например, $dup\\_ranks = [2]$ для примера, описанного выше."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"F5VwySUB6j2J"},"outputs":[],"source":["def hits_count(dup_ranks, k):\n","    \"\"\"\n","        dup_ranks: list индексов дубликатов\n","        result: вернуть  Hits@k\n","    \"\"\"\n","    sub_sum = 0\n","    for dup_rank in dup_ranks:\n","        if dup_rank <= k:\n","            sub_sum += 1\n","        else:\n","            sub_sum += 0\n","    return sub_sum / len(dup_ranks)\n"," "]},{"cell_type":"code","execution_count":11,"metadata":{"id":"82hQaxCH6j2R"},"outputs":[],"source":["def dcg_score(dup_ranks, k):\n","    \"\"\"\n","        dup_ranks: list индексов дубликатов\n","        result: вернуть DCG@k\n","    \"\"\"\n","    sub_sum = 0\n","    for dup_rank in dup_ranks:\n","        if dup_rank <= k:\n","            sub = 1 / np.log2(1 + dup_rank)\n","        else:\n","            sub = 0\n","        sub_sum += sub\n","    return sub_sum / len(dup_ranks)"]},{"cell_type":"markdown","metadata":{"id":"PcwHeXN26j2Y"},"source":["Протестируем функции. Пусть $N = 1$, то есть один эксперимент. Будем искать копию вопроса и оценивать метрики."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"fjISmOEW6j2h"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def cosine_sim(v1, v2):\n","    # v1, v2 (200,)\n","    similarity = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n","    return np.nan_to_num(similarity, nan=0)  # replace NaN with 0"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"gLa_Wqfh6j2m"},"outputs":[{"name":"stdout","output_type":"stream","text":["sim_ind 1 with similarity 0.99999994\n","Ваш ответ HIT: [0.0, 1.0, 1.0, 1.0]\n","Ваш ответ DCG: [0.0, 0.63093, 0.63093, 0.63093]\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\Anna\\AppData\\Local\\Temp\\ipykernel_9252\\704737424.py:3: RuntimeWarning: invalid value encountered in scalar divide\n","  similarity = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n"]}],"source":["copy_answers = [\"How does the catch keyword determine the type of exception that was thrown\",]\n","\n","# наши кандидаты\n","candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\",\n","                       \"How does the catch keyword determine the type of exception that was thrown\",\n","                       \"NSLog array description not memory address\",\n","                       \"PECL_HTTP not recognised php ubuntu\"],]\n","\n","# dup_ranks — позиции наших копий, так как эксперимент один, то этот массив длины 1\n","emb_origin = question_to_vec(copy_answers[0], wv_embeddings, tokenizer, dim=200)\n","similrity_rank = []\n","for i in candidates_ranking[0]:\n","    emb_copy = question_to_vec(i, wv_embeddings, tokenizer, dim=200)\n","    similrity_rank.append(cosine_sim(emb_origin, emb_copy))\n","\n","sim_ind = similrity_rank.index(max(similrity_rank))\n","print('sim_ind', sim_ind, 'with similarity', max(similrity_rank))\n","#дубликат был первым в ранжированном списке\n","dup_ranks = [sim_ind + 1]\n","\n","# вычисляем метрику для разных k\n","print('Ваш ответ HIT:', [hits_count(dup_ranks, k) for k in range(1, 5)])\n","print('Ваш ответ DCG:', [round(dcg_score(dup_ranks, k), 5) for k in range(1, 5)])"]},{"cell_type":"markdown","metadata":{"id":"MoHC3YoQ6j2t"},"source":["У вас должно получиться"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"B0NFWq4f6j2u","scrolled":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>HITS</th>\n","      <td>0</td>\n","      <td>1.00000</td>\n","      <td>1.00000</td>\n","      <td>1.00000</td>\n","    </tr>\n","    <tr>\n","      <th>DCG</th>\n","      <td>0</td>\n","      <td>0.63093</td>\n","      <td>0.63093</td>\n","      <td>0.63093</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      1        2        3        4\n","HITS  0  1.00000  1.00000  1.00000\n","DCG   0  0.63093  0.63093  0.63093"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# correct_answers - метрика для разных k\n","correct_answers = pd.DataFrame([[0, 1, 1, 1], [0, 1 / (np.log2(3)), 1 / (np.log2(3)), 1 / (np.log2(3))]],\n","                               index=['HITS', 'DCG'], columns=range(1,5))\n","correct_answers"]},{"cell_type":"markdown","metadata":{"id":"tHZqgDTo6j0i"},"source":["### Данные\n","[arxiv link](https://drive.google.com/file/d/1QqT4D0EoqJTy7v9VrNCYD-m964XZFR7_/edit)\n","\n","`train.tsv` - выборка для обучения.<br> В каждой строке через табуляцию записаны: **<вопрос>, <похожий вопрос>**\n","\n","`validation.tsv` - тестовая выборка.<br> В каждой строке через табуляцию записаны: **<вопрос>, <похожий вопрос>, <отрицательный пример 1>, <отрицательный пример 2>, ...**"]},{"cell_type":"markdown","metadata":{"id":"hil2UsUG6j22"},"source":["Считайте данные."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"B4EBho8s6j26"},"outputs":[],"source":["def read_corpus(filename):\n","    data = []\n","    for line in open(filename, encoding='utf-8'):\n","        data.append(line.split('\\t'))\n","    return data"]},{"cell_type":"markdown","metadata":{"id":"kkTxY3Mk9_nG"},"source":["Нам понадобиться только файл validation."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":274},"executionInfo":{"elapsed":21,"status":"error","timestamp":1628256058355,"user":{"displayName":"Deep Learning School","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNf0RkP5WvkU5MixKfC1Sv3mb-9QWgAbC6VcfQvA=s64","userId":"16549096980415837553"},"user_tz":-180},"id":"PTVB9Tnp6j29","outputId":"9c55c802-3d82-471d-eab2-195dabf5026c"},"outputs":[],"source":["validation_data = read_corpus(r'E:\\Iliushina_files\\DLS\\data\\validation.tsv')"]},{"cell_type":"markdown","metadata":{"id":"bTHfL-9y6j3F"},"source":["Кол-во строк"]},{"cell_type":"code","execution_count":107,"metadata":{"id":"z6ubXhIe6j3H","scrolled":false},"outputs":[{"data":{"text/plain":["3760"]},"execution_count":107,"metadata":{},"output_type":"execute_result"}],"source":["len(validation_data)"]},{"cell_type":"markdown","metadata":{"id":"kaOQblBy6j3M"},"source":["Размер нескольких первых строк"]},{"cell_type":"code","execution_count":108,"metadata":{"id":"yRx6e-Pe6j3M"},"outputs":[{"name":"stdout","output_type":"stream","text":["1 1001\n","2 1001\n","3 1001\n","4 1001\n","5 1001\n"]}],"source":["for i in range(5):\n","    print(i + 1, len(validation_data[i]))"]},{"cell_type":"markdown","metadata":{"id":"ySQQp0oQt1Ep"},"source":["### Ранжирование без обучения"]},{"cell_type":"markdown","metadata":{"id":"iElEDhj-6j3R"},"source":["Реализуйте функцию ранжирования кандидатов на основе косинусного расстояния. Функция должна по списку кандидатов вернуть отсортированный список пар (позиция в исходном списке кандидатов, кандидат). При этом позиция кандидата в полученном списке является его рейтингом (первый - лучший). Например, если исходный список кандидатов был [a, b, c], и самый похожий на исходный вопрос среди них - c, затем a, и в конце b, то функция должна вернуть список **[(2, c), (0, a), (1, b)]**."]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","def rank_candidates(question, candidates, embeddings, tokenizer, dim=200):\n","    \"\"\"\n","        question: строка\n","        candidates: массив строк(кандидатов) [a, b, c]\n","        result: пары (начальная позиция, кандидат) [(2, c), (0, a), (1, b)]\n","    \"\"\"\n","    pairs = []\n","    emb_question = question_to_vec(question, embeddings, tokenizer, dim)\n","    for i, candidate in enumerate(candidates):\n","        emb_cand = question_to_vec(candidate, embeddings, tokenizer, dim)\n","        pairs.append((i, candidate, cosine_similarity([emb_question], [emb_cand])[0][0]))\n","    return [pair[0:2] for pair in sorted(pairs, key=lambda x: x[2], reverse=True)]"]},{"cell_type":"markdown","metadata":{"id":"TnBszTb76j3c"},"source":["Протестируйте работу функции на примерах ниже. Пусть $N=2$, то есть два эксперимента"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"xvQgtP176j3h"},"outputs":[],"source":["questions = ['converting string to list', 'Sending array via Ajax fails'] \n","\n","candidates = [['Convert Google results object (pure js) to Python object', # первый эксперимент\n","               'C# create cookie from string and send it',\n","               'How to use jQuery AJAX for an outside domain?'],\n","              \n","              ['Getting all list items of an unordered list in PHP',      # второй эксперимент\n","               'WPF- How to update the changes in list item of a list',\n","               'select2 not displaying search results']]"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"bPj1JGFi6j3m"},"outputs":[{"name":"stdout","output_type":"stream","text":["[(1, 'C# create cookie from string and send it'), (0, 'Convert Google results object (pure js) to Python object'), (2, 'How to use jQuery AJAX for an outside domain?')]\n","\n","[(1, 'WPF- How to update the changes in list item of a list'), (0, 'Getting all list items of an unordered list in PHP'), (2, 'select2 not displaying search results')]\n","\n"]}],"source":["for question, q_candidates in zip(questions, candidates):\n","        ranks = rank_candidates(question, q_candidates, wv_embeddings, tokenizer)\n","        print(ranks)\n","        print()"]},{"cell_type":"markdown","metadata":{"id":"jm4cidj56j3q"},"source":["Для первого экперимента вы можете полностью сравнить ваши ответы и правильные ответы. Но для второго эксперимента два ответа на кандидаты будут <b>скрыты</b>(*)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0LeKMIsn6j3s"},"outputs":[],"source":["# должно вывести\n","results = [[(1, 'C# create cookie from string and send it'),\n","            (0, 'Convert Google results object (pure js) to Python object'),\n","            (2, 'How to use jQuery AJAX for an outside domain?')],\n","           [(*, 'Getting all list items of an unordered list in PHP'), #скрыт\n","            (*, 'select2 not displaying search results'), #скрыт\n","            (*, 'WPF- How to update the changes in list item of a list')]] #скрыт"]},{"cell_type":"markdown","metadata":{"id":"t1ttnIBe6j3x"},"source":["Последовательность начальных индексов вы должны получить `для эксперимента 1`  \n","Ответ : 1, 0, 2."]},{"cell_type":"markdown","metadata":{"id":"5WQgYDWd6j3y"},"source":["#### Вопрос 5:\n","* Какую последовательность начальных индексов вы получили `для эксперимента 2`(перечисление без запятой и пробелов, например, `102` для первого эксперимента?\n","\n","Ответ: 102"]},{"cell_type":"markdown","metadata":{"id":"fPllOY-Y6j30"},"source":["Теперь мы можем оценить качество нашего метода. Запустите следующие два блока кода для получения результата. Обратите внимание, что вычисление расстояния между векторами занимает некоторое время (примерно 10 минут). Можете взять для validation 1000 примеров."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"Z3q9sxddz-yU"},"outputs":[],"source":["from tqdm import tqdm\n","from nltk.tokenize import WordPunctTokenizer"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"nu7K4mis6j32"},"outputs":[{"name":"stdout","output_type":"stream","text":["<__main__.MyTokenizer object at 0x000001B47E24E660>\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1000/3760 [02:24<06:38,  6.93it/s]\n","100%|██████████| 6/6 [00:00<00:00, 461.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["DCG@   1: 0.285 | Hits@   1: 0.285\n","DCG@   5: 0.342 | Hits@   5: 0.393\n","DCG@  10: 0.360 | Hits@  10: 0.449\n","DCG@ 100: 0.406 | Hits@ 100: 0.679\n","DCG@ 500: 0.431 | Hits@ 500: 0.879\n","DCG@1000: 0.444 | Hits@1000: 1.000\n","---------------\n","WordPunctTokenizer(pattern='\\\\w+|[^\\\\w\\\\s]+', gaps=False, discard_empty=True, flags=re.UNICODE|re.MULTILINE|re.DOTALL)\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1000/3760 [02:20<06:26,  7.13it/s]\n","100%|██████████| 6/6 [00:00<00:00, 999.71it/s]"]},{"name":"stdout","output_type":"stream","text":["DCG@   1: 0.281 | Hits@   1: 0.281\n","DCG@   5: 0.337 | Hits@   5: 0.390\n","DCG@  10: 0.354 | Hits@  10: 0.444\n","DCG@ 100: 0.400 | Hits@ 100: 0.675\n","DCG@ 500: 0.426 | Hits@ 500: 0.877\n","DCG@1000: 0.439 | Hits@1000: 1.000\n","---------------\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["wv_ranking = []\n","max_validation_examples = 1000\n","for tokenizer in [MyTokenizer(), WordPunctTokenizer()]:\n","    print(tokenizer)\n","    for i, line in enumerate(tqdm(validation_data)):\n","        if i == max_validation_examples:\n","            break\n","        q, *ex = line\n","        ranks = rank_candidates(q, ex, wv_embeddings, tokenizer)\n","        wv_ranking.append([r[0] for r in ranks].index(0) + 1)\n","    for k in tqdm([1, 5, 10, 100, 500, 1000]):\n","        print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))\n","    print('---------------')"]},{"cell_type":"markdown","metadata":{"id":"LL6_Rjg3InL8"},"source":["### Эмбеддинги, обученные на корпусе похожих вопросов"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"iNvbpR5gJIPz"},"outputs":[],"source":["train_data = read_corpus(r'E:\\Iliushina_files\\DLS\\data\\train.tsv')"]},{"cell_type":"markdown","metadata":{"id":"Nr281ZyEJfjT"},"source":["Улучшите качество модели.<br>Склеим вопросы в пары и обучим на них модель Word2Vec из gensim. Выберите размер window. Объясните свой выбор."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"QuJzAM0cI-UH"},"outputs":[],"source":["from gensim.models import Word2Vec\n","proc_words = [tokenizer.tokenize(text) for text in list(zip(*train_data))[0] + list(zip(*train_data))[1]]"]},{"cell_type":"markdown","metadata":{},"source":["Нам важно определить контекстуальную информацию, а не локальную, то бОльшие значения window нам подходят больше. Аналогично с min_count. Нужно игнорировать редкие слова и сосредоточиться на более частых ключевых словах, поэтому min_count "]},{"cell_type":"code","execution_count":36,"metadata":{"id":"OQonbm4nMenD"},"outputs":[{"name":"stdout","output_type":"stream","text":["min_count: 1 , window: 1\n","31.425585985183716\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1000/3760 [02:12<06:06,  7.53it/s]\n","100%|██████████| 6/6 [00:00<00:00, 2000.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["DCG@   1: 0.272 | Hits@   1: 0.272\n","DCG@   5: 0.340 | Hits@   5: 0.403\n","DCG@  10: 0.358 | Hits@  10: 0.457\n","DCG@ 100: 0.403 | Hits@ 100: 0.677\n","DCG@ 500: 0.429 | Hits@ 500: 0.880\n","DCG@1000: 0.441 | Hits@1000: 1.000\n","------------------------\n","min_count: 1 , window: 3\n","51.01634740829468\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1000/3760 [02:13<06:07,  7.52it/s]\n","100%|██████████| 6/6 [00:00<00:00, 1999.03it/s]\n"]},{"name":"stdout","output_type":"stream","text":["DCG@   1: 0.310 | Hits@   1: 0.310\n","DCG@   5: 0.389 | Hits@   5: 0.460\n","DCG@  10: 0.407 | Hits@  10: 0.514\n","DCG@ 100: 0.450 | Hits@ 100: 0.729\n","DCG@ 500: 0.473 | Hits@ 500: 0.908\n","DCG@1000: 0.483 | Hits@1000: 1.000\n","------------------------\n","min_count: 1 , window: 6\n","69.96009635925293\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1000/3760 [02:13<06:07,  7.50it/s]\n","100%|██████████| 6/6 [00:00<00:00, 1999.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["DCG@   1: 0.342 | Hits@   1: 0.342\n","DCG@   5: 0.415 | Hits@   5: 0.481\n","DCG@  10: 0.436 | Hits@  10: 0.546\n","DCG@ 100: 0.479 | Hits@ 100: 0.757\n","DCG@ 500: 0.499 | Hits@ 500: 0.911\n","DCG@1000: 0.508 | Hits@1000: 1.000\n","------------------------\n","min_count: 1 , window: 10\n","82.62211489677429\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1000/3760 [02:13<06:07,  7.51it/s]\n","100%|██████████| 6/6 [00:00<00:00, 1499.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["DCG@   1: 0.342 | Hits@   1: 0.342\n","DCG@   5: 0.424 | Hits@   5: 0.496\n","DCG@  10: 0.444 | Hits@  10: 0.558\n","DCG@ 100: 0.485 | Hits@ 100: 0.766\n","DCG@ 500: 0.505 | Hits@ 500: 0.914\n","DCG@1000: 0.514 | Hits@1000: 1.000\n","------------------------\n","min_count: 3 , window: 1\n","26.89656710624695\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1000/3760 [02:13<06:08,  7.49it/s]\n","100%|██████████| 6/6 [00:00<00:00, 1999.03it/s]\n"]},{"name":"stdout","output_type":"stream","text":["DCG@   1: 0.288 | Hits@   1: 0.288\n","DCG@   5: 0.355 | Hits@   5: 0.415\n","DCG@  10: 0.375 | Hits@  10: 0.477\n","DCG@ 100: 0.420 | Hits@ 100: 0.695\n","DCG@ 500: 0.446 | Hits@ 500: 0.902\n","DCG@1000: 0.456 | Hits@1000: 1.000\n","------------------------\n","min_count: 3 , window: 3\n","44.56357479095459\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1000/3760 [02:13<06:09,  7.48it/s]\n","100%|██████████| 6/6 [00:00<00:00, 1499.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["DCG@   1: 0.329 | Hits@   1: 0.329\n","DCG@   5: 0.403 | Hits@   5: 0.471\n","DCG@  10: 0.423 | Hits@  10: 0.535\n","DCG@ 100: 0.466 | Hits@ 100: 0.745\n","DCG@ 500: 0.490 | Hits@ 500: 0.929\n","DCG@1000: 0.498 | Hits@1000: 1.000\n","------------------------\n","min_count: 3 , window: 6\n","59.14695453643799\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1000/3760 [02:12<06:06,  7.53it/s]\n","100%|██████████| 6/6 [00:00<00:00, 1499.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["DCG@   1: 0.340 | Hits@   1: 0.340\n","DCG@   5: 0.428 | Hits@   5: 0.505\n","DCG@  10: 0.448 | Hits@  10: 0.567\n","DCG@ 100: 0.493 | Hits@ 100: 0.782\n","DCG@ 500: 0.513 | Hits@ 500: 0.942\n","DCG@1000: 0.519 | Hits@1000: 1.000\n","------------------------\n","min_count: 3 , window: 10\n","70.77472949028015\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1000/3760 [02:13<06:07,  7.51it/s]\n","100%|██████████| 6/6 [00:00<00:00, 1999.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["DCG@   1: 0.364 | Hits@   1: 0.364\n","DCG@   5: 0.446 | Hits@   5: 0.521\n","DCG@  10: 0.465 | Hits@  10: 0.580\n","DCG@ 100: 0.508 | Hits@ 100: 0.786\n","DCG@ 500: 0.529 | Hits@ 500: 0.945\n","DCG@1000: 0.534 | Hits@1000: 1.000\n","------------------------\n","min_count: 6 , window: 1\n","26.23577308654785\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1000/3760 [02:13<06:09,  7.47it/s]\n","100%|██████████| 6/6 [00:00<00:00, 1999.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["DCG@   1: 0.299 | Hits@   1: 0.299\n","DCG@   5: 0.364 | Hits@   5: 0.425\n","DCG@  10: 0.385 | Hits@  10: 0.491\n","DCG@ 100: 0.429 | Hits@ 100: 0.708\n","DCG@ 500: 0.455 | Hits@ 500: 0.911\n","DCG@1000: 0.465 | Hits@1000: 1.000\n","------------------------\n","min_count: 6 , window: 3\n","42.025683879852295\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1000/3760 [02:12<06:06,  7.53it/s]\n","100%|██████████| 6/6 [00:00<00:00, 1499.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["DCG@   1: 0.339 | Hits@   1: 0.339\n","DCG@   5: 0.412 | Hits@   5: 0.476\n","DCG@  10: 0.436 | Hits@  10: 0.549\n","DCG@ 100: 0.479 | Hits@ 100: 0.760\n","DCG@ 500: 0.501 | Hits@ 500: 0.934\n","DCG@1000: 0.508 | Hits@1000: 1.000\n","------------------------\n","min_count: 6 , window: 6\n","57.28208923339844\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1000/3760 [02:12<06:06,  7.53it/s]\n","100%|██████████| 6/6 [00:00<00:00, 2000.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["DCG@   1: 0.348 | Hits@   1: 0.348\n","DCG@   5: 0.432 | Hits@   5: 0.505\n","DCG@  10: 0.457 | Hits@  10: 0.581\n","DCG@ 100: 0.500 | Hits@ 100: 0.791\n","DCG@ 500: 0.519 | Hits@ 500: 0.942\n","DCG@1000: 0.525 | Hits@1000: 1.000\n","------------------------\n","min_count: 6 , window: 10\n","68.07311773300171\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1000/3760 [02:13<06:07,  7.52it/s]\n","100%|██████████| 6/6 [00:00<00:00, 1999.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["DCG@   1: 0.367 | Hits@   1: 0.367\n","DCG@   5: 0.454 | Hits@   5: 0.529\n","DCG@  10: 0.477 | Hits@  10: 0.599\n","DCG@ 100: 0.518 | Hits@ 100: 0.800\n","DCG@ 500: 0.537 | Hits@ 500: 0.951\n","DCG@1000: 0.542 | Hits@1000: 1.000\n","------------------------\n","min_count: 10 , window: 1\n","23.906816005706787\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1000/3760 [02:12<06:06,  7.53it/s]\n","100%|██████████| 6/6 [00:00<00:00, 2000.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["DCG@   1: 0.300 | Hits@   1: 0.300\n","DCG@   5: 0.368 | Hits@   5: 0.431\n","DCG@  10: 0.393 | Hits@  10: 0.510\n","DCG@ 100: 0.435 | Hits@ 100: 0.719\n","DCG@ 500: 0.459 | Hits@ 500: 0.912\n","DCG@1000: 0.469 | Hits@1000: 1.000\n","------------------------\n","min_count: 10 , window: 3\n","39.52173852920532\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1000/3760 [02:13<06:07,  7.51it/s]\n","100%|██████████| 6/6 [00:00<00:00, 1999.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["DCG@   1: 0.341 | Hits@   1: 0.341\n","DCG@   5: 0.419 | Hits@   5: 0.488\n","DCG@  10: 0.442 | Hits@  10: 0.560\n","DCG@ 100: 0.484 | Hits@ 100: 0.765\n","DCG@ 500: 0.507 | Hits@ 500: 0.940\n","DCG@1000: 0.513 | Hits@1000: 1.000\n","------------------------\n","min_count: 10 , window: 6\n","53.8204026222229\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1000/3760 [02:13<06:08,  7.49it/s]\n","100%|██████████| 6/6 [00:00<00:00, 1499.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["DCG@   1: 0.353 | Hits@   1: 0.353\n","DCG@   5: 0.443 | Hits@   5: 0.520\n","DCG@  10: 0.463 | Hits@  10: 0.582\n","DCG@ 100: 0.508 | Hits@ 100: 0.797\n","DCG@ 500: 0.528 | Hits@ 500: 0.955\n","DCG@1000: 0.533 | Hits@1000: 1.000\n","------------------------\n","min_count: 10 , window: 10\n","67.7125928401947\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1000/3760 [02:24<06:38,  6.93it/s]\n","100%|██████████| 6/6 [00:00<00:00, 1999.67it/s]"]},{"name":"stdout","output_type":"stream","text":["DCG@   1: 0.372 | Hits@   1: 0.372\n","DCG@   5: 0.454 | Hits@   5: 0.526\n","DCG@  10: 0.478 | Hits@  10: 0.600\n","DCG@ 100: 0.521 | Hits@ 100: 0.813\n","DCG@ 500: 0.539 | Hits@ 500: 0.954\n","DCG@1000: 0.544 | Hits@1000: 1.000\n","------------------------\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import time\n","\n","for min_count in [1, 3,6,10]:\n","    for window in [1,3,6,10]:\n","        print('min_count:', min_count, ', window:', window)\n","        current_time = time.time()\n","        embeddings_trained = Word2Vec(proc_words, # data for model to train on\n","                        vector_size=200,                 # embedding vector size\n","                        min_count=min_count,             # consider words that occured at least 5 times\n","                        workers=4,\n","                        window=window,\n","                        sg=1).wv\n","        end_time = time.time()\n","        print(end_time-current_time)\n","        wv_ranking = []\n","        max_validation_examples = 1000\n","        tokenizer = MyTokenizer()\n","        for i, line in enumerate(tqdm(validation_data)):\n","            if i == max_validation_examples:\n","                break\n","            q, *ex = line\n","            ranks = rank_candidates(q, ex, embeddings_trained, tokenizer)\n","            wv_ranking.append([r[0] for r in ranks].index(0) + 1)\n","\n","        for k in tqdm([1, 5, 10, 100, 500, 1000]):\n","            print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))\n","        print('------------------------')"]},{"cell_type":"markdown","metadata":{},"source":["чем больше window тем лучше, доказано результатами, хоть и занимает больше времени(70сек вместо 20-40сек). При переборе параметров, наибольшее качество было достигнуто при min_count: 10 , window: 10"]},{"cell_type":"markdown","metadata":{},"source":["#### Далее изучим как ведут себя различные токенизаторы при выбранных гиперпараметрах"]},{"cell_type":"markdown","metadata":{},"source":["##### TreebankWordTokenizer"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1000/3760 [02:44<07:33,  6.08it/s]\n","100%|██████████| 6/6 [00:00<00:00, 1499.75it/s]"]},{"name":"stdout","output_type":"stream","text":["DCG@   1: 0.332 | Hits@   1: 0.332\n","DCG@   5: 0.417 | Hits@   5: 0.490\n","DCG@  10: 0.438 | Hits@  10: 0.554\n","DCG@ 100: 0.481 | Hits@ 100: 0.766\n","DCG@ 500: 0.504 | Hits@ 500: 0.940\n","DCG@1000: 0.510 | Hits@1000: 1.000\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from nltk.tokenize import TreebankWordTokenizer\n","\n","current_time = time.time()\n","tokenizer = TreebankWordTokenizer()\n","\n","proc_words = [tokenizer.tokenize(text) for text in list(zip(*train_data))[0] + list(zip(*train_data))[1]]\n","embeddings_trained = Word2Vec(proc_words, # data for model to train on\n","                        vector_size=200,                 # embedding vector size\n","                        min_count=10,             # consider words that occured at least 5 times\n","                        workers=4,\n","                        window=10,\n","                        sg=1).wv\n","end_time = time.time()\n","print('time for model train:', end_time-current_time, ' sec')\n","wv_ranking = []\n","max_validation_examples = 1000\n","\n","for i, line in enumerate(tqdm(validation_data)):\n","    if i == max_validation_examples:\n","        break\n","    q, *ex = line\n","    ranks = rank_candidates(q, ex, embeddings_trained, tokenizer)\n","    wv_ranking.append([r[0] for r in ranks].index(0) + 1)\n","\n","for k in tqdm([1, 5, 10, 100, 500, 1000]):\n","    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))"]},{"cell_type":"markdown","metadata":{},"source":["##### WordPunctTokenizer"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["time for model train: 81.41124892234802  sec\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1000/3760 [02:16<06:15,  7.35it/s]\n","100%|██████████| 6/6 [00:00<00:00, 1499.84it/s]"]},{"name":"stdout","output_type":"stream","text":["DCG@   1: 0.352 | Hits@   1: 0.352\n","DCG@   5: 0.442 | Hits@   5: 0.520\n","DCG@  10: 0.463 | Hits@  10: 0.585\n","DCG@ 100: 0.504 | Hits@ 100: 0.786\n","DCG@ 500: 0.525 | Hits@ 500: 0.947\n","DCG@1000: 0.530 | Hits@1000: 1.000\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from nltk.tokenize import WordPunctTokenizer\n","import time\n","from gensim.models import Word2Vec\n","\n","current_time = time.time()\n","tokenizer = WordPunctTokenizer()\n","\n","proc_words = [tokenizer.tokenize(text) for text in list(zip(*train_data))[0] + list(zip(*train_data))[1]]\n","embeddings_trained = Word2Vec(proc_words, # data for model to train on\n","                        vector_size=200,                 # embedding vector size\n","                        min_count=10,             # consider words that occured at least 5 times\n","                        workers=4,\n","                        window=10,\n","                        sg=1).wv\n","end_time = time.time()\n","print('time for model train:', end_time-current_time, ' sec')\n","wv_ranking = []\n","max_validation_examples = 1000\n","\n","for i, line in enumerate(tqdm(validation_data)):\n","    if i == max_validation_examples:\n","        break\n","    q, *ex = line\n","    ranks = rank_candidates(q, ex, embeddings_trained, tokenizer)\n","    wv_ranking.append([r[0] for r in ranks].index(0) + 1)\n","\n","for k in tqdm([1, 5, 10, 100, 500, 1000]):\n","    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))"]},{"cell_type":"markdown","metadata":{},"source":["лучше всего работает простой токенайзер"]},{"cell_type":"markdown","metadata":{},"source":["#### с нормализацией"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["couple_list = [couple[0] + str(' ') + couple[1] for couple in train_data]"]},{"cell_type":"markdown","metadata":{},"source":["##### Spacy Lemmatizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import spacy\n","import spacy.cli\n","spacy.cli.download(\"en_core_web_sm\")\n","nlp = spacy.load(\"en_core_web_sm\")\n","words_lemma = [[token.lemma_ for token in nlp(text)]\n","              for text in tqdm(couple_list)]\n","#слишком долго, обработка около 2,5 часов"]},{"cell_type":"markdown","metadata":{},"source":["##### nltk Lemmatizer"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1000000/1000000 [00:30<00:00, 32405.50it/s]\n"]}],"source":["from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","words_lemma = [[lemmatizer.lemmatize(word) for word in MyTokenizer().tokenize(text)]\n","              for text in tqdm(couple_list)]\n","#это работает меньше чем за минуту"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":["from gensim.models import Word2Vec\n","embeddings_trained = Word2Vec(words_lemma, # data for model to train on\n","                        vector_size=200,                 # embedding vector size\n","                        min_count=10,             # consider words that occured at least 5 times\n","                        workers=4,\n","                        window=10,\n","                        sg=1).wv"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1000/3760 [02:23<06:35,  6.98it/s]\n","100%|██████████| 6/6 [00:00<00:00, 1998.87it/s]"]},{"name":"stdout","output_type":"stream","text":["DCG@   1: 0.409 | Hits@   1: 0.409\n","DCG@   5: 0.491 | Hits@   5: 0.566\n","DCG@  10: 0.513 | Hits@  10: 0.634\n","DCG@ 100: 0.551 | Hits@ 100: 0.827\n","DCG@ 500: 0.568 | Hits@ 500: 0.953\n","DCG@1000: 0.573 | Hits@1000: 1.000\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["wv_ranking = []\n","max_validation_examples = 1000\n","\n","for i, line in enumerate(tqdm(validation_data)):\n","    if i == max_validation_examples:\n","        break\n","    q, *ex = line\n","    ranks = rank_candidates(q, ex, embeddings_trained, MyTokenizer())\n","    wv_ranking.append([r[0] for r in ranks].index(0) + 1)\n","\n","for k in tqdm([1, 5, 10, 100, 500, 1000]):\n","    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))"]},{"cell_type":"markdown","metadata":{},"source":["##### pymorphy3 Lemmatizer"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1000000/1000000 [02:00<00:00, 8330.97it/s]\n"]}],"source":["import pymorphy3\n","morph = pymorphy3.MorphAnalyzer()\n","words_lemma_morphy3 = [[morph.parse(word)[0].normal_form for word in MyTokenizer().tokenize(text)]\n","              for text in tqdm(couple_list)]\n","#сработало за 2 минуты"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":["from gensim.models import Word2Vec\n","embeddings_trained = Word2Vec(words_lemma_morphy3, # data for model to train on\n","                        vector_size=200,                 # embedding vector size\n","                        min_count=10,             # consider words that occured at least 5 times\n","                        workers=4,\n","                        window=10,\n","                        sg=1).wv"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 1000/3760 [02:16<06:16,  7.33it/s]\n","100%|██████████| 6/6 [00:00<00:00, 1999.35it/s]"]},{"name":"stdout","output_type":"stream","text":["DCG@   1: 0.264 | Hits@   1: 0.264\n","DCG@   5: 0.325 | Hits@   5: 0.380\n","DCG@  10: 0.341 | Hits@  10: 0.430\n","DCG@ 100: 0.382 | Hits@ 100: 0.631\n","DCG@ 500: 0.410 | Hits@ 500: 0.849\n","DCG@1000: 0.425 | Hits@1000: 1.000\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["wv_ranking = []\n","max_validation_examples = 1000\n","\n","for i, line in enumerate(tqdm(validation_data)):\n","    if i == max_validation_examples:\n","        break\n","    q, *ex = line\n","    ranks = rank_candidates(q, ex, embeddings_trained, MyTokenizer())\n","    wv_ranking.append([r[0] for r in ranks].index(0) + 1)\n","\n","for k in tqdm([1, 5, 10, 100, 500, 1000]):\n","    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))"]},{"cell_type":"markdown","metadata":{"id":"tY8PxB0j-ThG"},"source":["### Замечание:\n","Решить эту задачу с помощью обучения полноценной нейронной сети будет вам предложено, как часть задания в одной из домашних работ по теме \"Диалоговые системы\"."]},{"cell_type":"markdown","metadata":{"id":"vymVj8IxO2PO"},"source":["Напишите свой вывод о полученных результатах.\n","* Какой принцип токенизации даёт качество лучше и почему?\n","* Помогает ли нормализация слов?\n","* Какие эмбеддинги лучше справляются с задачей и почему?\n","* Почему получилось плохое качество решения задачи?\n","* Предложите свой подход к решению задачи.\n","\n","## Вывод:\n"]},{"cell_type":"markdown","metadata":{"id":"emODHztAQUQz"},"source":["\n","* Простой токенайзер показал наилучшую точность. MyTokenizer (DCG@1000: 0.544) TreebankWordTokenizer(0.510) WordPunctTokenizer(0.530). Вероятнее всего это из-за того, что WordPunctTokenizer учитывает знаки препинания и регистр, а TreebankWordTokenizer еще сложнее и разбиваниет сокращения, притяжательные местоимения и дефисные слова на отдельные слова.\n","* Нормализация слов улучшила результат: DCG@1000: 0.573 при nltk.WordNetLemmatizer и MyTokenizer. При этом и обучение Word2Vec стало значительно дольше(2 минуты). Тестирование Spacy Lemmatizer показало, что его использование занимает сликом много времени на обработку слов (2,5 часа). \n","* Эмбединги, обученные на корпусе вопроса, позволяют достичь большей точности, так как учитвают контекст вопроса (DCG@1000: 0.544 vs 0.444). Также было вывлено, что архитектура типа Skip-gram(sg=1) где контекстные слова предсказываются с использованием базового слова дает лучшее качество обучения, чем CBoW (результаты не сохранились, но этот параметр проверялся). Наилучшими для тренировки Word2Vec оказались эмбединги с min_count=10, window=10, потому что это позволяет оценивать весь контекст предложения. Гипер-параметр min_count не так сильно влиял на качество по среванению с гипер-параметром window.\n","* Полученные DCG Scores являются хорошими и модель способна эфективно ранжировать большинство случаев, однако в случае сложных кейсов часто ошибается. Для улучшения качества решения задачи можно использовать более современные модели. Также мера косинусного расстояния может быть не подходящей для этой задачи, можно потестировать Manhattan distance\n","* Можно побольше потестировать различные токенизаторы и убрать стоп-слова или слова паразиты"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"colab":{"collapsed_sections":["BIWqBuEa6j0b","uS9FwWNd5a3S","MQk_rolFwT_h","ai48-5vv6j1d","Y60z4t6W6j16","0sUSxk866j1_","J5xWOORI6j2F","tHZqgDTo6j0i","ySQQp0oQt1Ep","LL6_Rjg3InL8"],"name":"[homework]simple_embeddings.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"}},"nbformat":4,"nbformat_minor":0}
